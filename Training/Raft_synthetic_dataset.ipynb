{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9361198,"sourceType":"datasetVersion","datasetId":5675934},{"sourceId":9376186,"sourceType":"datasetVersion","datasetId":5687395},{"sourceId":28808,"sourceType":"modelInstanceVersion","modelInstanceId":8332,"modelId":3301},{"sourceId":104449,"sourceType":"modelInstanceVersion","modelInstanceId":68809,"modelId":91102},{"sourceId":104453,"sourceType":"modelInstanceVersion","modelInstanceId":72256,"modelId":76277}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Importing Files**","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Fetching PDF's through API**","metadata":{}},{"cell_type":"code","source":"!pip install feedparser\n!pip install PyPDF2\nimport requests\nimport feedparser\nfrom PyPDF2 import PdfReader\nimport io\n\ndef search_arxiv(query, max_results=50):\n    base_url = \"http://export.arxiv.org/api/query?\"\n    search_query = f\"search_query={query}&max_results={max_results}\"\n    url = base_url + search_query\n    response = requests.get(url)\n    feed = feedparser.parse(response.content)\n    return feed.entries\n\ndef extract_text_from_pdf(pdf_url):\n    response = requests.get(pdf_url)\n    pdf_file = io.BytesIO(response.content)\n    \n    reader = PdfReader(pdf_file)\n    total_text = \"\"\n    total_pages = len(reader.pages)\n    \n    for i in range(total_pages):\n        page = reader.pages[i]\n        text = page.extract_text()\n        if text:\n            total_text += text + \" \"\n    \n    return total_text\n\ndef get_combined_text_for_nlp_papers():\n    query = \"nlp\"  # You can adjust this query as per your requirements\n    nlp_papers = search_arxiv(query, max_results=50)\n    \n    combined_text = \"\"\n    \n    for paper in nlp_papers:\n        pdf_url = paper.get(\"pdf_url\", paper.link.replace('abs', 'pdf')) + \".pdf\"\n        print(f\"Fetching and extracting from: {paper.title}\")\n        \n        try:\n            paper_text = extract_text_from_pdf(pdf_url)\n            combined_text += paper_text + \" \"\n        except Exception as e:\n            print(f\"Error processing {pdf_url}: {e}\")\n    \n    return combined_text\n\ncombined_nlp_text = get_combined_text_for_nlp_papers()\nprint(f\"Extracted combined text length: {len(combined_nlp_text)}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-18T05:43:07.560176Z","iopub.execute_input":"2024-10-18T05:43:07.560660Z","iopub.status.idle":"2024-10-18T05:45:54.410848Z","shell.execute_reply.started":"2024-10-18T05:43:07.560615Z","shell.execute_reply":"2024-10-18T05:45:54.409466Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting feedparser\n  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\nCollecting sgmllib3k (from feedparser)\n  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hDownloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=9e5997bf522e351b41a069f71f94e18657311786aeae19f37c7fa75ffba65036\n  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\nSuccessfully built sgmllib3k\nInstalling collected packages: sgmllib3k, feedparser\nSuccessfully installed feedparser-6.0.11 sgmllib3k-1.0.0\nCollecting PyPDF2\n  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\nDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: PyPDF2\nSuccessfully installed PyPDF2-3.0.1\nFetching and extracting from: Spark NLP: Natural Language Understanding at Scale\nFetching and extracting from: Training an NLP Scholar at a Small Liberal Arts College: A Backwards\n  Designed Course Proposal\nFetching and extracting from: Towards Systematic Monolingual NLP Surveys: GenA of Greek NLP\nFetching and extracting from: Sejarah dan Perkembangan Teknik Natural Language Processing (NLP) Bahasa\n  Indonesia: Tinjauan tentang sejarah, perkembangan teknologi, dan aplikasi NLP\n  dalam bahasa Indonesia\nFetching and extracting from: The Law and NLP: Bridging Disciplinary Disconnects\nFetching and extracting from: A Survey on Recognizing Textual Entailment as an NLP Evaluation\nFetching and extracting from: A Survey of Race, Racism, and Anti-Racism in NLP\nFetching and extracting from: Large Language Models Meet NLP: A Survey\nFetching and extracting from: NLP Methods in Host-based Intrusion Detection Systems: A Systematic\n  Review and Future Directions\nFetching and extracting from: We are Who We Cite: Bridges of Influence Between Natural Language\n  Processing and Other Academic Fields\nFetching and extracting from: Notes on Deep Learning for NLP\nFetching and extracting from: Natural Language Processing 4 All (NLP4All): A New Online Platform for\n  Teaching and Learning NLP Concepts\nFetching and extracting from: Classification of Natural Language Processing Techniques for\n  Requirements Engineering\nFetching and extracting from: Translational NLP: A New Paradigm and General Principles for Natural\n  Language Processing Research\nFetching and extracting from: NusaCrowd: A Call for Open and Reproducible NLP Research in Indonesian\n  Languages\nFetching and extracting from: A Partial Exact Penalty Function Approach for Constrained Optimization\nFetching and extracting from: A Review of Digital Learning Environments for Teaching Natural Language\n  Processing in K-12 Education\nFetching and extracting from: The Nature of NLP: Analyzing Contributions in NLP Papers\nFetching and extracting from: The NLP Sandbox: an efficient model-to-data system to enable federated\n  and unbiased evaluation of clinical NLP models\nFetching and extracting from: An NLP Assistant for Clide\nFetching and extracting from: The Current State of Finnish NLP\nFetching and extracting from: Improving Interpretability via Explicit Word Interaction Graph Layer\nFetching and extracting from: Towards a Holistic Approach: Understanding Sociodemographic Biases in\n  NLP Models using an Interdisciplinary Lens\nFetching and extracting from: Ling-CL: Understanding NLP Models through Linguistic Curricula\nFetching and extracting from: Shoulders of Giants: A Look at the Degree and Utility of Openness in NLP\n  Research\nFetching and extracting from: How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social\n  Impact\nFetching and extracting from: Large-Scale News Classification using BERT Language Model: Spark NLP\n  Approach\nFetching and extracting from: A Systematic Review of Natural Language Processing for Knowledge\n  Management in Healthcare\nFetching and extracting from: Teaching NLP outside Linguistics and Computer Science classrooms: Some\n  challenges and some opportunities\nFetching and extracting from: Model Explainability in Deep Learning Based Natural Language Processing\nFetching and extracting from: An Empirical Survey of Data Augmentation for Limited Data Learning in\n  NLP\nFetching and extracting from: Towards Improving Adversarial Training of NLP Models\nFetching and extracting from: BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation\n  Models\nFetching and extracting from: Meta Learning for Natural Language Processing: A Survey\nFetching and extracting from: Differentially Private Natural Language Models: Recent Advances and\n  Future Directions\nFetching and extracting from: Synthesizing Human Gaze Feedback for Improved NLP Performance\nFetching and extracting from: On the Origins of Bias in NLP through the Lens of the Jim Code\nFetching and extracting from: Fairness Certification for Natural Language Processing and Large\n  Language Models\nFetching and extracting from: NLP for Knowledge Discovery and Information Extraction from Energetics\n  Corpora\nFetching and extracting from: NLP-KG: A System for Exploratory Search of Scientific Literature in\n  Natural Language Processing\nFetching and extracting from: On the Equivalence between Logic Programming and SETAF\nFetching and extracting from: Noisy Text Data: Achilles' Heel of popular transformer based NLP models\nFetching and extracting from: STAMP 4 NLP -- An Agile Framework for Rapid Quality-Driven NLP\n  Applications Development\nFetching and extracting from: From Insights to Actions: The Impact of Interpretability and Analysis\n  Research on NLP\nFetching and extracting from: Representation Learning for Natural Language Processing\nFetching and extracting from: Applied Language Technology: NLP for the Humanities\nFetching and extracting from: Appendix - Recommended Statistical Significance Tests for NLP Tasks\nFetching and extracting from: Efficient transfer learning for NLP with ELECTRA\nFetching and extracting from: Natural Language Processing: State of The Art, Current Trends and\n  Challenges\nFetching and extracting from: Towards Linguistically Generalizable NLP Systems: A Workshop and Shared\n  Task\nExtracted combined text length: 3956343\n","output_type":"stream"}]},{"cell_type":"code","source":"total_text = combined_nlp_text\nprint(len(total_text))","metadata":{"execution":{"iopub.status.busy":"2024-10-18T05:53:32.159232Z","iopub.execute_input":"2024-10-18T05:53:32.159780Z","iopub.status.idle":"2024-10-18T05:53:32.168192Z","shell.execute_reply.started":"2024-10-18T05:53:32.159732Z","shell.execute_reply":"2024-10-18T05:53:32.166750Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"3956343\n","output_type":"stream"}]},{"cell_type":"code","source":"total_text = total_text.replace('\"', '')\ntotal_text = total_text.replace(\"{\", '')\ntotal_text = total_text.replace(\"}\", '')\nprint(len(total_text))","metadata":{"execution":{"iopub.status.busy":"2024-10-18T05:53:34.016395Z","iopub.execute_input":"2024-10-18T05:53:34.016867Z","iopub.status.idle":"2024-10-18T05:53:34.095744Z","shell.execute_reply.started":"2024-10-18T05:53:34.016808Z","shell.execute_reply":"2024-10-18T05:53:34.093931Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"3954565\n","output_type":"stream"}]},{"cell_type":"code","source":"chunk_size = 150\ntotal_text_array = total_text.split()\nchunks = []\n\nchunk = \"\"\nfor i in range(0, len(total_text_array)):\n  chunk = chunk + total_text_array[i] + \" \"\n  if(i != 0 and (i % chunk_size == 0 or i == len(total_text_array) - 1)):\n    chunks.append(chunk)\n    chunk = \"\"","metadata":{"execution":{"iopub.status.busy":"2024-10-18T05:54:04.935231Z","iopub.execute_input":"2024-10-18T05:54:04.936556Z","iopub.status.idle":"2024-10-18T05:54:05.672266Z","shell.execute_reply.started":"2024-10-18T05:54:04.936493Z","shell.execute_reply":"2024-10-18T05:54:05.671182Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print(len(chunks))","metadata":{"execution":{"iopub.status.busy":"2024-10-18T05:54:07.612689Z","iopub.execute_input":"2024-10-18T05:54:07.613797Z","iopub.status.idle":"2024-10-18T05:54:07.619653Z","shell.execute_reply.started":"2024-10-18T05:54:07.613747Z","shell.execute_reply":"2024-10-18T05:54:07.618364Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"3926\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\nwith open('/kaggle/working/chunks.json', 'w') as f:\n    json.dump(chunks, f)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T05:55:02.538302Z","iopub.execute_input":"2024-10-18T05:55:02.538768Z","iopub.status.idle":"2024-10-18T05:55:02.582982Z","shell.execute_reply.started":"2024-10-18T05:55:02.538722Z","shell.execute_reply":"2024-10-18T05:55:02.581641Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# **Generating dataset using Gemma model**","metadata":{}},{"cell_type":"code","source":"#General format\n{\n  'id': 'seed_task_0', \n  'type': 'general', \n  'question': 'What is the official motto of the United States of America?', \n  'context': {\n    'sentences': [\n      [\"the Gulf of Mexico are prone to hurricanes, ... and enforces the Act. [ 189 ] As of 2022, the U. S\",\n    \"energy from fossil fuel and the largest ... there are 19, 969 airports in the U. S., of which 5, 193 are designated\",\n    'weaponry, ideology, and international i... and is a permanent member of the UN Security Council. The first documentary evidence of the phrase \" United States',\n    '[CLS] United States of America Flag Coat of arms ... dominance in nuclear and conventional',\n    '##om ic soft pow er. [ 405 ] [ 406 ] Nearly all present ... rights in the United States are advanced by global standards.']\n    ],\n    'title': [\n      ['placeholder_title',\n      'placeholder_title',\n      'placeholder_title',\n      'placeholder_title',\n      'placeholder_title']\n    ]\n  },\n  'answer': '\"In God We Trust\"',\n  'cot_answer': None\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!!pip install -q -U accelerate\n!pip install peft","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\nimport transformers\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig\nfrom peft import prepare_model_for_kbit_training,LoraConfig,PeftModel,get_peft_model\nfrom datasets import load_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = \"/kaggle/input/gemma/transformers/7b-it/3\"\ndevice = \"cuda\" # the device to load the model onto\n\n# Quantization configuration\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n     #bnb_4bit_compute_dtype=\"float16\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Loading the model and tokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path,quantization_config=bnb_config)\ntokenizer = AutoTokenizer.from_pretrained(\n    model_path,\n    model_max_length=512,\n    padding_side=\"left\",\n    add_eos_token=True)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(chunks[40])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question_prompt = (\n    \"Based on the following text, generate exactly 5 short and relevant questions regarding a fact in the text. \"\n    \"The output should be a valid JSON object with 5 key-value pairs, where each key is 'question_X' (X being the number 1 to 5) \"\n    \"and each value is the corresponding question. Return only the JSON object in the following format:\\n\"\n    \"{\\n\\\"question_1\\\": \\\"<question 1>\\\",\\n\\\"question_2\\\": \\\"<question 2>\\\",\\n\\\"question_3\\\": \\\"<question 3>\\\",\\n\"\n    \"\\\"question_4\\\": \\\"<question 4>\\\",\\n\\\"question_5\\\": \\\"<question 5>\\\"\\n} Do not repeat or reference any part of the input text in your response, and only output the JSON. Stick to this format and strictly do not write or output anything else. Your response should just be a json object and no other text. If a text is in double quotes then put backslash to include it into json\\n\\n\"\n    + chunks[0] + \".\"\n)\nquestion_inputs = tokenizer(question_prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=10000).to(device)\nattention_mask = question_inputs[\"attention_mask\"].to(device)\n\noutputs = model.generate(question_inputs[\"input_ids\"], attention_mask=attention_mask, max_length=10000, pad_token_id=tokenizer.eos_token_id)\nquestion = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nstart_idx = question.find('{', question.find('{') + 1)\nend_idx = question.find('}', start_idx + 1)\n\nquestion_json = question[start_idx: end_idx + 1]\nprint(question_json)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answer_prompt = (\n    \"Based on the following text, generate exactly 5 relavant answers for the following questions regarding a fact in the text. \"\n    \"The output should be a valid JSON object with 5 key-value pairs, where each key is 'answer_X' (X being the number 1 to 5) \"\n    \"and each value is the corresponding answer. Return only the JSON object in the following format:\\n\"\n    \"{\\n\\\"answer_1\\\": \\\"<answer 1>\\\",\\n\\\"answer_2\\\": \\\"<answer 2>\\\",\\n\\\"answer_3\\\": \\\"<answer 3>\\\",\\n\"\n    \"\\\"answer_4\\\": \\\"<answer 4>\\\",\\n\\\"answer_5\\\": \\\"<answer 5>\\\"\\n} Do not repeat or reference any part of the input text in your response, and only output the JSON. Stick to this format and strictly do not write or output anything else. Your response should just be a json object and no other text\\n\\n\"\n    + chunks[0] + \".\" + \"\\n\\n\" + question_json + \".\"\n)\nanswer_inputs = tokenizer(answer_prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=10000).to(device)\nattention_mask = answer_inputs[\"attention_mask\"].to(device)\n\noutputs = model.generate(answer_inputs[\"input_ids\"], attention_mask=attention_mask, max_length=10000, pad_token_id=tokenizer.eos_token_id)\nanswer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nstart_idx = answer.find('{', answer.find('{', answer.find('{') + 1) + 1)\nend_idx = answer.find('}', start_idx + 1)\n\nanswer_json = answer[start_idx: end_idx + 1]\nprint(answer_json)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(chunks) / 20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 0\ndatapoints = []\nfor i,chunk in enumerate(chunks[:197]):\n    print(f\"Chunk: {i}\")\n    #Creating a dictionary of 5 questions\n    question_prompt = (\n    \"Based on the following text, generate exactly 5 short and relevant questions regarding a fact in the text. \"\n    \"The output should be a valid JSON object with 5 key-value pairs, where each key is 'question_X' (X being the number 1 to 5) \"\n    \"and each value is the corresponding question. Return only the JSON object in the following format:\\n\"\n    \"{\\n\\\"question_1\\\": \\\"<question 1>\\\",\\n\\\"question_2\\\": \\\"<question 2>\\\",\\n\\\"question_3\\\": \\\"<question 3>\\\",\\n\"\n    \"\\\"question_4\\\": \\\"<question 4>\\\",\\n\\\"question_5\\\": \\\"<question 5>\\\"\\n} Do not repeat or reference any part of the input text in your response, and only output the JSON. Stick to this format and strictly do not write or output anything else. Your response should just be a json object and no other text\\n\\n\"\n    + chunk + \".\"\n    )\n    question_inputs = tokenizer(question_prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=4000).to(device)\n    attention_mask = question_inputs[\"attention_mask\"].to(device)\n\n    outputs = model.generate(question_inputs[\"input_ids\"], attention_mask=attention_mask, max_length=4000, pad_token_id=tokenizer.eos_token_id)\n    question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    start_idx = question.find('{', question.find('{') + 1)\n    end_idx = question.find('}', start_idx + 1)\n    questions = question[start_idx: end_idx + 1]\n    \n    try:\n        questions_json = json.loads(questions)\n    except json.JSONDecodeError:\n        print(f\"Skipping chunk {i} due to JSONDecodeError in question generation.\")\n        continue \n    \n    #Creating a dictionary of 5 answers\n    answer_prompt = (\n    \"Based on the following text, generate exactly 5 relavant answers for the following questions regarding a fact in the text. \"\n    \"The output should be a valid JSON object with 5 key-value pairs, where each key is 'answer_X' (X being the number 1 to 5) \"\n    \"and each value is the corresponding answer. Return only the JSON object in the following format:\\n\"\n    \"{\\n\\\"answer_1\\\": \\\"<answer 1>\\\",\\n\\\"answer_2\\\": \\\"<answer 2>\\\",\\n\\\"answer_3\\\": \\\"<answer 3>\\\",\\n\"\n    \"\\\"answer_4\\\": \\\"<answer 4>\\\",\\n\\\"answer_5\\\": \\\"<answer 5>\\\"\\n} Do not repeat or reference any part of the input text in your response, and only output the JSON. Stick to this format and strictly do not write or output anything else. Your response should just be a json object and no other text\\n\\n\"\n    + chunk + \".\" + \"\\n\\n\" + questions + \".\"\n    )\n    answer_inputs = tokenizer(answer_prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=4000).to(device)\n    attention_mask = answer_inputs[\"attention_mask\"].to(device)\n\n    outputs = model.generate(answer_inputs[\"input_ids\"], attention_mask=attention_mask, max_length=4000, pad_token_id=tokenizer.eos_token_id)\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    start_idx = answer.find('{', answer.find('{', answer.find('{') + 1) + 1)\n    end_idx = answer.find('}', start_idx + 1)\n    answers = answer[start_idx: end_idx + 1]\n    \n    try:\n        answers_json = json.loads(answers)\n    except json.JSONDecodeError:\n        print(f\"Skipping chunk {i} due to JSONDecodeError in question generation.\")\n        continue \n        \n    #Creating a datapoint for each question-answer pair\n    for i, j in zip(questions_json, answers_json):\n        random_chunks = random.choices(chunks, k = 4)\n        random_chunks.append(chunk)\n        datapoint = {\n            'id': 'seed_task_' + str(seed),\n            'type': 'general',\n            'question': questions_json[i],\n            'context': {\n                'sentences' : [\n                    random_chunks\n                ],\n                'title': [\n                    ['placeholder_title',\n                    'placeholder_title',\n                    'placeholder_title',\n                    'placeholder_title',\n                    'placeholder_title']\n                ]\n            },\n            'answer': answers_json[j],\n            'cot_answer': None\n        }\n        datapoints.append(datapoint)\n        seed = seed + 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(datapoints))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('nlp-197.json', 'w') as f:\n    json.dump(datapoints, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}